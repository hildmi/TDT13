{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                     ID  \\\n",
      "0  691324c4-5c30-44e0-b9e4-45b4f0715e21   \n",
      "1  d4295391-9ca5-4398-b7c8-687e4a984ef1   \n",
      "2  58937fa5-3c2c-426b-8255-5a140fbab675   \n",
      "3  7daf364c-3b33-4cbe-be37-a214edf9a73e   \n",
      "4  22518271-4bb4-4caf-b683-7305da519288   \n",
      "\n",
      "                                                post class_name  class_id  \n",
      "0  i was making questions for my students and i r...       none         5  \n",
      "1  i've recently requested testing accommodations...       adhd         0  \n",
      "2  **cambodia** * koh rong: amazing beaches and a...       none         5  \n",
      "3  synesthesia. what is synesthesia? according to...       none         5  \n",
      "4  iâ€™m phil baran and i teach organic chemistry a...       none         5  \n",
      "class_id\n",
      "0    2465\n",
      "3    2450\n",
      "1    2422\n",
      "2    2407\n",
      "4    2001\n",
      "5    1982\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "train_df = pd.read_csv('posts_train.csv')\n",
    "\n",
    "# Explore the first few records\n",
    "print(train_df.head())\n",
    "\n",
    "# Explore the distribution of classes\n",
    "print(train_df['class_id'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import XLNetTokenizer\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Load the datasets\n",
    "train_df = pd.read_csv('posts_train.csv')\n",
    "val_df = pd.read_csv('posts_val.csv')\n",
    "test_df = pd.read_csv('posts_test.csv')\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = XLNetTokenizer.from_pretrained('xlnet-base-cased')\n",
    "\n",
    "# Tokenize your data\n",
    "train_encodings = tokenizer(train_df['post'].tolist(), truncation=True, padding=True, max_length=512)\n",
    "val_encodings = tokenizer(val_df['post'].tolist(), truncation=True, padding=True, max_length=512)\n",
    "test_encodings = tokenizer(test_df['post'].tolist(), truncation=True, padding=True, max_length=512)\n",
    "\n",
    "#Labels\n",
    "train_labels = train_df['class_id'].values\n",
    "val_labels = val_df['class_id'].values\n",
    "test_labels = test_df['class_id'].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class RedditDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "# Create the dataset objects\n",
    "train_dataset = RedditDataset(train_encodings, train_labels)\n",
    "val_dataset = RedditDataset(val_encodings, val_labels)\n",
    "test_dataset = RedditDataset(test_encodings, test_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['logits_proj.weight', 'logits_proj.bias', 'sequence_summary.summary.bias', 'sequence_summary.summary.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "Using the `Trainer` with `PyTorch` requires `accelerate>=0.20.1`: Please run `pip install transformers[torch]` or `pip install accelerate -U`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Load pre-trained XLNet model with a classification head\u001b[39;00m\n\u001b[1;32m      4\u001b[0m model \u001b[38;5;241m=\u001b[39m XLNetForSequenceClassification\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mxlnet-base-cased\u001b[39m\u001b[38;5;124m'\u001b[39m, num_labels\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m6\u001b[39m)  \u001b[38;5;66;03m# We have 6 classes\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m training_args \u001b[38;5;241m=\u001b[39m \u001b[43mTrainingArguments\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mper_device_train_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mper_device_eval_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_train_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevaluation_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mepoch\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10_000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_total_limit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mremove_unused_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./results\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogging_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./logs\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogging_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m200\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m     20\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     21\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[1;32m     22\u001b[0m     train_dataset\u001b[38;5;241m=\u001b[39mtrain_dataset,\n\u001b[1;32m     23\u001b[0m     eval_dataset\u001b[38;5;241m=\u001b[39mval_dataset,\n\u001b[1;32m     24\u001b[0m )\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n",
      "File \u001b[0;32m<string>:115\u001b[0m, in \u001b[0;36m__init__\u001b[0;34m(self, output_dir, overwrite_output_dir, do_train, do_eval, do_predict, evaluation_strategy, prediction_loss_only, per_device_train_batch_size, per_device_eval_batch_size, per_gpu_train_batch_size, per_gpu_eval_batch_size, gradient_accumulation_steps, eval_accumulation_steps, eval_delay, learning_rate, weight_decay, adam_beta1, adam_beta2, adam_epsilon, max_grad_norm, num_train_epochs, max_steps, lr_scheduler_type, warmup_ratio, warmup_steps, log_level, log_level_replica, log_on_each_node, logging_dir, logging_strategy, logging_first_step, logging_steps, logging_nan_inf_filter, save_strategy, save_steps, save_total_limit, save_safetensors, save_on_each_node, no_cuda, use_cpu, use_mps_device, seed, data_seed, jit_mode_eval, use_ipex, bf16, fp16, fp16_opt_level, half_precision_backend, bf16_full_eval, fp16_full_eval, tf32, local_rank, ddp_backend, tpu_num_cores, tpu_metrics_debug, debug, dataloader_drop_last, eval_steps, dataloader_num_workers, past_index, run_name, disable_tqdm, remove_unused_columns, label_names, load_best_model_at_end, metric_for_best_model, greater_is_better, ignore_data_skip, sharded_ddp, fsdp, fsdp_min_num_params, fsdp_config, fsdp_transformer_layer_cls_to_wrap, deepspeed, label_smoothing_factor, optim, optim_args, adafactor, group_by_length, length_column_name, report_to, ddp_find_unused_parameters, ddp_bucket_cap_mb, ddp_broadcast_buffers, dataloader_pin_memory, skip_memory_metrics, use_legacy_prediction_loop, push_to_hub, resume_from_checkpoint, hub_model_id, hub_strategy, hub_token, hub_private_repo, hub_always_push, gradient_checkpointing, include_inputs_for_metrics, fp16_backend, push_to_hub_model_id, push_to_hub_organization, push_to_hub_token, mp_parameters, auto_find_batch_size, full_determinism, torchdynamo, ray_scope, ddp_timeout, torch_compile, torch_compile_backend, torch_compile_mode, dispatch_batches, include_tokens_per_second)\u001b[0m\n",
      "File \u001b[0;32m~/Desktop/tdt13ny/TDT13/myenv/lib/python3.11/site-packages/transformers/training_args.py:1436\u001b[0m, in \u001b[0;36mTrainingArguments.__post_init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   <a href='file:///Users/varasheim/Desktop/tdt13ny/TDT13/myenv/lib/python3.11/site-packages/transformers/training_args.py?line=1429'>1430</a>\u001b[0m     \u001b[39mif\u001b[39;00m version\u001b[39m.\u001b[39mparse(version\u001b[39m.\u001b[39mparse(torch\u001b[39m.\u001b[39m__version__)\u001b[39m.\u001b[39mbase_version) \u001b[39m==\u001b[39m version\u001b[39m.\u001b[39mparse(\u001b[39m\"\u001b[39m\u001b[39m2.0.0\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfp16:\n\u001b[1;32m   <a href='file:///Users/varasheim/Desktop/tdt13ny/TDT13/myenv/lib/python3.11/site-packages/transformers/training_args.py?line=1430'>1431</a>\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m--optim adamw_torch_fused with --fp16 requires PyTorch>2.0\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   <a href='file:///Users/varasheim/Desktop/tdt13ny/TDT13/myenv/lib/python3.11/site-packages/transformers/training_args.py?line=1432'>1433</a>\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m   <a href='file:///Users/varasheim/Desktop/tdt13ny/TDT13/myenv/lib/python3.11/site-packages/transformers/training_args.py?line=1433'>1434</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mframework \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   <a href='file:///Users/varasheim/Desktop/tdt13ny/TDT13/myenv/lib/python3.11/site-packages/transformers/training_args.py?line=1434'>1435</a>\u001b[0m     \u001b[39mand\u001b[39;00m is_torch_available()\n\u001b[0;32m-> <a href='file:///Users/varasheim/Desktop/tdt13ny/TDT13/myenv/lib/python3.11/site-packages/transformers/training_args.py?line=1435'>1436</a>\u001b[0m     \u001b[39mand\u001b[39;00m (\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdevice\u001b[39m.\u001b[39mtype \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   <a href='file:///Users/varasheim/Desktop/tdt13ny/TDT13/myenv/lib/python3.11/site-packages/transformers/training_args.py?line=1436'>1437</a>\u001b[0m     \u001b[39mand\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice\u001b[39m.\u001b[39mtype \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mnpu\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   <a href='file:///Users/varasheim/Desktop/tdt13ny/TDT13/myenv/lib/python3.11/site-packages/transformers/training_args.py?line=1437'>1438</a>\u001b[0m     \u001b[39mand\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice\u001b[39m.\u001b[39mtype \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxpu\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   <a href='file:///Users/varasheim/Desktop/tdt13ny/TDT13/myenv/lib/python3.11/site-packages/transformers/training_args.py?line=1438'>1439</a>\u001b[0m     \u001b[39mand\u001b[39;00m (get_xla_device_type(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice) \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mGPU\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   <a href='file:///Users/varasheim/Desktop/tdt13ny/TDT13/myenv/lib/python3.11/site-packages/transformers/training_args.py?line=1439'>1440</a>\u001b[0m     \u001b[39mand\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfp16 \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfp16_full_eval)\n\u001b[1;32m   <a href='file:///Users/varasheim/Desktop/tdt13ny/TDT13/myenv/lib/python3.11/site-packages/transformers/training_args.py?line=1440'>1441</a>\u001b[0m ):\n\u001b[1;32m   <a href='file:///Users/varasheim/Desktop/tdt13ny/TDT13/myenv/lib/python3.11/site-packages/transformers/training_args.py?line=1441'>1442</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   <a href='file:///Users/varasheim/Desktop/tdt13ny/TDT13/myenv/lib/python3.11/site-packages/transformers/training_args.py?line=1442'>1443</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mFP16 Mixed precision training with AMP or APEX (`--fp16`) and FP16 half precision evaluation\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   <a href='file:///Users/varasheim/Desktop/tdt13ny/TDT13/myenv/lib/python3.11/site-packages/transformers/training_args.py?line=1443'>1444</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m (`--fp16_full_eval`) can only be used on CUDA or NPU devices or certain XPU devices (with IPEX).\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   <a href='file:///Users/varasheim/Desktop/tdt13ny/TDT13/myenv/lib/python3.11/site-packages/transformers/training_args.py?line=1444'>1445</a>\u001b[0m     )\n\u001b[1;32m   <a href='file:///Users/varasheim/Desktop/tdt13ny/TDT13/myenv/lib/python3.11/site-packages/transformers/training_args.py?line=1446'>1447</a>\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m   <a href='file:///Users/varasheim/Desktop/tdt13ny/TDT13/myenv/lib/python3.11/site-packages/transformers/training_args.py?line=1447'>1448</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mframework \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   <a href='file:///Users/varasheim/Desktop/tdt13ny/TDT13/myenv/lib/python3.11/site-packages/transformers/training_args.py?line=1448'>1449</a>\u001b[0m     \u001b[39mand\u001b[39;00m is_torch_available()\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   <a href='file:///Users/varasheim/Desktop/tdt13ny/TDT13/myenv/lib/python3.11/site-packages/transformers/training_args.py?line=1455'>1456</a>\u001b[0m     \u001b[39mand\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbf16 \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbf16_full_eval)\n\u001b[1;32m   <a href='file:///Users/varasheim/Desktop/tdt13ny/TDT13/myenv/lib/python3.11/site-packages/transformers/training_args.py?line=1456'>1457</a>\u001b[0m ):\n",
      "File \u001b[0;32m~/Desktop/tdt13ny/TDT13/myenv/lib/python3.11/site-packages/transformers/training_args.py:1901\u001b[0m, in \u001b[0;36mTrainingArguments.device\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   <a href='file:///Users/varasheim/Desktop/tdt13ny/TDT13/myenv/lib/python3.11/site-packages/transformers/training_args.py?line=1896'>1897</a>\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/varasheim/Desktop/tdt13ny/TDT13/myenv/lib/python3.11/site-packages/transformers/training_args.py?line=1897'>1898</a>\u001b[0m \u001b[39mThe device used by this process.\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/varasheim/Desktop/tdt13ny/TDT13/myenv/lib/python3.11/site-packages/transformers/training_args.py?line=1898'>1899</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/varasheim/Desktop/tdt13ny/TDT13/myenv/lib/python3.11/site-packages/transformers/training_args.py?line=1899'>1900</a>\u001b[0m requires_backends(\u001b[39mself\u001b[39m, [\u001b[39m\"\u001b[39m\u001b[39mtorch\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[0;32m-> <a href='file:///Users/varasheim/Desktop/tdt13ny/TDT13/myenv/lib/python3.11/site-packages/transformers/training_args.py?line=1900'>1901</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_setup_devices\n",
      "File \u001b[0;32m~/Desktop/tdt13ny/TDT13/myenv/lib/python3.11/site-packages/transformers/utils/generic.py:54\u001b[0m, in \u001b[0;36mcached_property.__get__\u001b[0;34m(self, obj, objtype)\u001b[0m\n\u001b[1;32m     <a href='file:///Users/varasheim/Desktop/tdt13ny/TDT13/myenv/lib/python3.11/site-packages/transformers/utils/generic.py?line=51'>52</a>\u001b[0m cached \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(obj, attr, \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m     <a href='file:///Users/varasheim/Desktop/tdt13ny/TDT13/myenv/lib/python3.11/site-packages/transformers/utils/generic.py?line=52'>53</a>\u001b[0m \u001b[39mif\u001b[39;00m cached \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m---> <a href='file:///Users/varasheim/Desktop/tdt13ny/TDT13/myenv/lib/python3.11/site-packages/transformers/utils/generic.py?line=53'>54</a>\u001b[0m     cached \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfget(obj)\n\u001b[1;32m     <a href='file:///Users/varasheim/Desktop/tdt13ny/TDT13/myenv/lib/python3.11/site-packages/transformers/utils/generic.py?line=54'>55</a>\u001b[0m     \u001b[39msetattr\u001b[39m(obj, attr, cached)\n\u001b[1;32m     <a href='file:///Users/varasheim/Desktop/tdt13ny/TDT13/myenv/lib/python3.11/site-packages/transformers/utils/generic.py?line=55'>56</a>\u001b[0m \u001b[39mreturn\u001b[39;00m cached\n",
      "File \u001b[0;32m~/Desktop/tdt13ny/TDT13/myenv/lib/python3.11/site-packages/transformers/training_args.py:1801\u001b[0m, in \u001b[0;36mTrainingArguments._setup_devices\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   <a href='file:///Users/varasheim/Desktop/tdt13ny/TDT13/myenv/lib/python3.11/site-packages/transformers/training_args.py?line=1798'>1799</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m is_sagemaker_mp_enabled():\n\u001b[1;32m   <a href='file:///Users/varasheim/Desktop/tdt13ny/TDT13/myenv/lib/python3.11/site-packages/transformers/training_args.py?line=1799'>1800</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m is_accelerate_available(min_version\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m0.20.1\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m-> <a href='file:///Users/varasheim/Desktop/tdt13ny/TDT13/myenv/lib/python3.11/site-packages/transformers/training_args.py?line=1800'>1801</a>\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mImportError\u001b[39;00m(\n\u001b[1;32m   <a href='file:///Users/varasheim/Desktop/tdt13ny/TDT13/myenv/lib/python3.11/site-packages/transformers/training_args.py?line=1801'>1802</a>\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mUsing the `Trainer` with `PyTorch` requires `accelerate>=0.20.1`: Please run `pip install transformers[torch]` or `pip install accelerate -U`\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   <a href='file:///Users/varasheim/Desktop/tdt13ny/TDT13/myenv/lib/python3.11/site-packages/transformers/training_args.py?line=1802'>1803</a>\u001b[0m         )\n\u001b[1;32m   <a href='file:///Users/varasheim/Desktop/tdt13ny/TDT13/myenv/lib/python3.11/site-packages/transformers/training_args.py?line=1803'>1804</a>\u001b[0m     AcceleratorState\u001b[39m.\u001b[39m_reset_state(reset_partial_state\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m   <a href='file:///Users/varasheim/Desktop/tdt13ny/TDT13/myenv/lib/python3.11/site-packages/transformers/training_args.py?line=1804'>1805</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdistributed_state \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[0;31mImportError\u001b[0m: Using the `Trainer` with `PyTorch` requires `accelerate>=0.20.1`: Please run `pip install transformers[torch]` or `pip install accelerate -U`"
     ]
    }
   ],
   "source": [
    "from transformers import XLNetForSequenceClassification, Trainer, TrainingArguments\n",
    "\n",
    "# Load pre-trained XLNet model with a classification head\n",
    "model = XLNetForSequenceClassification.from_pretrained('xlnet-base-cased', num_labels=6)  # We have 6 classes\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=64,\n",
    "    num_train_epochs=3,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_steps=10_000,\n",
    "    save_total_limit=2,\n",
    "    remove_unused_columns=False,\n",
    "    output_dir='./results',\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=200,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "eval_results = trainer.evaluate()\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
